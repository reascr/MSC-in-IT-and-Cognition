{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Processing 1: assignment 2\n",
    "\n",
    "**Deadline: Nov 16, 23:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment has three parts:\n",
    "\n",
    "In the first part, you will have to analyze and extend the Minimum Edit Distance algorithm to return an alignment, and not only the minimum number of operations **(20% of the grade)**.\n",
    "\n",
    "In the second part, you will train some language recognizers based on language models and you will be asked to find out which are the errors that the model is making **(20% of the grade)**.\n",
    "\n",
    "The last part will be about sentiment analysis, where you will implement the Naive Bayes model that we saw in  class **(60% of the grade)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Getting an alignment using Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we give you an implementation of Minimum Edit Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1 1 - 2.0 2.0 0.0 s\n",
      "1.0 1 2 - 3.0 1.0 3.0 d\n",
      "2.0 1 3 - 4.0 2.0 4.0 d\n",
      "3.0 1 4 - 5.0 3.0 5.0 d\n",
      "4.0 1 5 - 6.0 4.0 4.0 d\n",
      "1.0 2 1 - 1.0 3.0 3.0 i\n",
      "2.0 2 2 - 2.0 2.0 2.0 i\n",
      "3.0 2 3 - 3.0 3.0 3.0 i\n",
      "2.0 2 4 - 4.0 4.0 2.0 s\n",
      "3.0 2 5 - 5.0 3.0 5.0 d\n",
      "2.0 3 1 - 2.0 4.0 2.0 i\n",
      "3.0 3 2 - 3.0 3.0 3.0 i\n",
      "4.0 3 3 - 4.0 4.0 4.0 i\n",
      "3.0 3 4 - 3.0 5.0 5.0 i\n",
      "2.0 3 5 - 4.0 4.0 2.0 s\n",
      "[[5. 4. 3. 2.]\n",
      " [4. 3. 2. 3.]\n",
      " [3. 2. 3. 4.]\n",
      " [2. 1. 2. 3.]\n",
      " [1. 0. 1. 2.]\n",
      " [0. 1. 2. 3.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops=['i','d','s']\n",
    "\n",
    "def MED (src_str,trg_str):\n",
    "    src_str = \"#\"+src_str\n",
    "    trg_str = \"#\"+trg_str\n",
    "    \n",
    "    ins_cost = 1\n",
    "    del_cost = 1\n",
    "    sub_cost = 2\n",
    "    \n",
    "    m = len(src_str)\n",
    "    n = len(trg_str)\n",
    "    \n",
    "    #INITIALIZE DISTANCE MATRIX WITH ZEROS\n",
    "    distance_matrix = np.zeros((n,m))\n",
    "\n",
    "    #INITIALIZE COLUMN 0 VALUES\n",
    "    distance_matrix [:,0] = np.arange(0,n,del_cost)\n",
    "    #INITIALIZE ROW 0 VALUES\n",
    "    distance_matrix [0,:] = np.arange(0,m,ins_cost)    \n",
    "    \n",
    "    for i in range(1,n): #each column\n",
    "        for j in range(1,m): #each row\n",
    "            insert = distance_matrix[i-1,j] + ins_cost\n",
    "            delete = distance_matrix[i,j-1] + del_cost\n",
    "            if src_str[j]==trg_str[i]:\n",
    "                substi = distance_matrix[i-1,j-1]\n",
    "            else:\n",
    "                substi = distance_matrix[i-1,j-1] + sub_cost\n",
    "\n",
    "            distance_matrix[i,j] = min([insert,delete,substi])\n",
    "            which_op = np.argmin([insert,delete,substi])\n",
    "            \n",
    "            print (distance_matrix[i,j],i,j, \"-\",insert,delete,substi, ops[which_op])\n",
    "            \n",
    "            \n",
    "    #If you uncomment the following command you can see\n",
    "    #the distance matrix in the same way\n",
    "    #that appears in Figure 3.27 from the SLP2 book, page 111.\n",
    "    print (np.flip(distance_matrix.T, axis=0))\n",
    "    \n",
    "    #RETURN THE LAST ELEMENT\n",
    "    return distance_matrix[-1,-1]\n",
    "\n",
    "MED (\"PRNAP\",\"PAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1 (20 pts):\n",
    "\n",
    "You have to extend the given implementation so that it returns one alignment. Considering one minimum edit distance, there might be more than one alignment. If you return one possible alignment, the exercise will be considered correct.\n",
    "\n",
    "The alignment should be a sequence of actions that should be applied to the source string. Considering the source string \"intention\" and the target string \"execution\", the function could return\n",
    "\n",
    "`['d','s','s','-','i','s','-','-','-','-']`\n",
    "\n",
    "where `d` represents deletion, `i` represents insertion, `s` represents substitution and `-` represents no change. This result could be used to represent the two aligned strings as in Figure 3.23 in SLP2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.0,\n",
       " array([['', '', '', '', '', '', '', '', '', ''],\n",
       "        ['', 'i', 'i', 'i', 's', 'd', 'd', 'd', 'd', 'd'],\n",
       "        ['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i'],\n",
       "        ['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i'],\n",
       "        ['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i'],\n",
       "        ['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i'],\n",
       "        ['', 'i', 'i', 's', 'i', 'i', 's', 'd', 'd', 'd'],\n",
       "        ['', 's', 'd', 'i', 'i', 'i', 'i', 's', 'd', 'd'],\n",
       "        ['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 's', 'd'],\n",
       "        ['', 'i', 's', 'd', 'd', 'd', 'i', 'i', 'i', 's']], dtype='<U1'),\n",
       " ['d', 'd', 'd', '-', 'd', 'i', 'i', 'i', 'i', '-', '-', '-', '-'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops=['i','d','s']\n",
    "def MED_alignment(src_str, trg_str):\n",
    "    src_str = \"#\"+src_str\n",
    "    trg_str = \"#\"+trg_str\n",
    "    \n",
    "    ins_cost = 1\n",
    "    del_cost = 1\n",
    "    sub_cost = 2\n",
    "    \n",
    "    m = len(src_str)\n",
    "    n = len(trg_str)\n",
    "    \n",
    "    #INITIALIZE DISTANCE MATRIX WITH ZEROS\n",
    "    distance_matrix = np.zeros((n,m))\n",
    "\n",
    "    #INITIALIZE COLUMN 0 VALUES\n",
    "    distance_matrix [:,0] = np.arange(0,n,del_cost)\n",
    "    #INITIALIZE ROW 0 VALUES\n",
    "    distance_matrix [0,:] = np.arange(0,m,ins_cost)\n",
    "    \n",
    "    bck = np.zeros((n,m),dtype=str)\n",
    "    \n",
    "    for i in range(1,n): #each column\n",
    "        for j in range(1,m): #each row\n",
    "            same_character = False\n",
    "            insert = distance_matrix[i-1,j] + ins_cost\n",
    "            delete = distance_matrix[i,j-1] + del_cost\n",
    "            if src_str[j]==trg_str[i]:\n",
    "                substi = distance_matrix[i-1,j-1]\n",
    "                same_character = True\n",
    "            else:\n",
    "                substi = distance_matrix[i-1,j-1] + sub_cost\n",
    "\n",
    "            distance_matrix[i,j] = min([insert,delete,substi])\n",
    "            which_op = np.argmin([insert,delete,substi]) \n",
    "            bck[i,j] = ops[which_op]\n",
    "            \n",
    "            #print (distance_matrix[i,j],i,j, \"-\",insert,delete,substi, ops[which_op])\n",
    "        \n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    op_seq = list() \n",
    "    # start with last cell (at the bottom right)\n",
    "    i = n-1 \n",
    "    j = m-1\n",
    "    while i>0 or j>0: # when both i and j are smaller than 0 the loop stops\n",
    "        #print(i,j)\n",
    "        if i == 0: # if we are in the first row add deletion and go left in the table\n",
    "            op_seq.append(\"d\")\n",
    "            j -=1\n",
    "        elif j == 0: # if we are in the first row add insertion and go up in the table\n",
    "            op_seq.append(\"i\")\n",
    "            i -=1\n",
    "        else:\n",
    "            if bck[i,j] == \"s\": \n",
    "                if src_str[j] == trg_str[i]: # if source and target string have the same character do nothing\n",
    "                    op_seq.append(\"-\")\n",
    "                else:\n",
    "                    op_seq.append(\"s\") # otherwise add substitution to list of sequences\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif bck[i,j] == \"i\": \n",
    "                op_seq.append(\"i\")\n",
    "                i -= 1 \n",
    "            else:\n",
    "                op_seq.append(\"d\")\n",
    "                j -=1\n",
    "        #print(op_seq)\n",
    "    op_seq = op_seq[::-1] # reverse order of operations\n",
    "    \n",
    "    \n",
    "    \n",
    "    #If you uncomment the following command you can see\n",
    "    #the distance matrix in the same way\n",
    "    #that appears in Figure 3.27 from the SLP2 book, page 111.\n",
    "    #print (np.flip(distance_matrix.T, axis=0))\n",
    "    \n",
    "    #return distance_matrix,bck, op_seq_dist\n",
    "    return distance_matrix[-1,-1],bck,op_seq\n",
    "MED_alignment (\"intention\",\"execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Language Models\n",
    "\n",
    "In the following cells of code, we show you a simple language recognizer based on written bibles, available on Github (https://github.com/christos-c/bible-corpus).\n",
    "\n",
    "Please download the whole repository (https://github.com/christos-c/bible-corpus). You can download it from the website and you should save a folder called bible-corpus.\n",
    "\n",
    "If you prefer, you can clone the repository by executing the following cell (This command will work for those that have a Unix based system and have Git installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: Zielpfad 'bible-corpus' existiert bereits und ist kein leeres Verzeichnis.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/christos-c/bible-corpus.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a folder called bible-corpus, which contains another folder called bibles with a number of XML files. Each of these XML files contains a bible in a specific language, encoded in XML format.\n",
    "\n",
    "The following piece of code will read some corpora. You can specify which languages you read by specifying their codes in the variable `languages`, separated by spaces. Now it loads Spanish(es), Danish(da), English(en), Basque(eu), Swedish(sv) and Finnish(fi) corpora and it saves each corpus in a dictionary called `corpus`. We can access each corpus, which is just raw text, by writing `corpus[\"es\"]` (this will get us the spanish corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................bible-corpus/bibles/Danish.xml\n",
      "i begyndelsen skabte gud himmelen og jorden. og jorden var øde og tom, og der var mørke over verdens\n",
      ".............bible-corpus/bibles/German.xml\n",
      "am anfang schuf gott himmel und erde. und die erde war wüst und leer, und es war finster auf der tie\n",
      "...................bible-corpus/bibles/English-WEB.xml\n",
      "in the beginning god created the heavens and the earth. now the earth was formless and empty. darkne\n",
      ".....................bible-corpus/bibles/Spanish.xml\n",
      "en el principio creó dios los cielos y la tierra y la tierra estaba sin orden y vacía. había tiniebl\n",
      ".......bible-corpus/bibles/Italian.xml\n",
      "in principio dio creò il cielo e la terra ora la terra era informe e deserta e le tenebre ricoprivan\n",
      ".............................bible-corpus/bibles/English.xml\n",
      "in the beginning god created the heaven and the earth. and the earth was without form, and void; and\n",
      "."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "languages = \"da de it es en\".split()\n",
    "\n",
    "\n",
    "corpus = {}\n",
    "for file in glob.glob(\"bible-corpus/bibles/*.xml\"):\n",
    "    print (\".\",end=\"\")\n",
    "    et = etree.parse(file)\n",
    "    r  = et.getroot()\n",
    "    \n",
    "    language = r.findall(\"cesHeader/profileDesc/langUsage/language\")[0].attrib['id']\n",
    "    \n",
    "    if language in languages:\n",
    "        print (file)\n",
    "        segs = r.findall(\"text/body/div/div/seg\")\n",
    "    \n",
    "        corpus[language]  = \" \".join([seg.text.strip().lower() for seg in segs if seg.text is not None])\n",
    "        print(corpus[language][:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the beginning god created the heaven and the ea'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at how the corpus starts in, e.g. English:\n",
    "\n",
    "corpus['en'][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, I made a simple program (`train_lm`) to train a language model given a text. Then I included a simple function `return_proba` that gets a probability distribution and a sentence, and it returns the probability of that sentence to belong to that language.\n",
    "\n",
    "If you want to know how this works, please feel free to check out [this simple notebook](https://absalon.ku.dk/files/7716003/download?download_frd=1) that shows how you can make a simple language recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm(data):\n",
    "    # data I/O\n",
    "    chars = list(set(data))\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "    print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "    cfreq_bigrams_corpus= nltk.ConditionalFreqDist(nltk.bigrams(data))\n",
    "    cprob_bigrams_corpus = nltk.ConditionalProbDist(cfreq_bigrams_corpus, nltk.MLEProbDist)\n",
    "    return cprob_bigrams_corpus\n",
    "\n",
    "def prod (ite):\n",
    "    if len(ite)==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return ite[0]*prod(ite[1:])\n",
    "    \n",
    "\n",
    "def return_proba (cpd, sentence):\n",
    "    return prod([cpd[bigram[0]].prob(bigram[1]) if cpd[bigram[0]].prob(bigram[1]) != 0 else 1 for bigram in nltk.bigrams(sentence)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a set of language models for the languages initially specified and it saves each distribution in a variable called `probability_distributions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da\n",
      "data has 3555654 characters, 55 unique.\n",
      "de\n",
      "data has 4035302 characters, 54 unique.\n",
      "it\n",
      "data has 3809385 characters, 56 unique.\n",
      "es\n",
      "data has 3912619 characters, 56 unique.\n",
      "en\n",
      "data has 4138511 characters, 37 unique.\n",
      "<ConditionalProbDist with 55 conditions>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probability_distributions = {}\n",
    "for language in languages:\n",
    "    print (language)\n",
    "    probability_distributions[language] = train_lm(corpus[language])\n",
    "print(probability_distributions[\"da\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see a function that gets a sentence and a probability distribution as input, and it returns a list of tuples. Each tuple contains a language and a probability of belonging to that language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Episoden med de mange skotske fans, der lagde et voldsomt tryk på udebanefansenes indgang til Brøndby Stadion, hvor politiet måtte trække stavene, har resulteret i én anholdelse.\".lower()\n",
    "\n",
    "def which_language(sentence, p_d):\n",
    "    return [(lang, return_proba(p_d[lang],sentence)) for lang in p_d.keys()] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('da', 1.546902713190505e-193),\n",
       " ('de', 2.4585571382154064e-208),\n",
       " ('it', 5.87111804766044e-215),\n",
       " ('es', 5.376541598888056e-210),\n",
       " ('en', 1.7619041469049169e-208)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can see in this case that the model is able to detect that the text is written in Danish,\n",
    "#as it is the language that shows the highest probability.\n",
    "\n",
    "which_language(sent,probability_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1 (20 pts):\n",
    "\n",
    "Select five languages from the corpus and train a language recognizer that can distinguish between those languages. The only thing that you have to do for this is to change the variable `languages` that on top with the languages that you desire to work with. The languages must have either the same or a similar alphabet.\n",
    "\n",
    "Once that you selected the languages that you want to work with, you should look for (at least) five examples that the model fails to get the correct language. And you should try to justify why that is happening. Why does the model fail to guess those languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Sentence 1: The model incorrectly predicts Danish instead of German. This could be caused by the use of loanwords that do not appear in the training data (\"Bug\" and \"gefixt\"). \n",
    " * Sentence 2: The model incorrectly predicts English instead of Italian. This could be caused by the use of propernames (\"Eduardo\"), numbers (\"120000\") and currencies (\"Euro\") that do not appear in the training data. \n",
    " * Sentence 3: The model incorrectly predicts German instead of English. This could be caused by the use of informal speech.\n",
    " * Sentence 4: The model incorrectly predicts Spanish instead of German. This could be caused by the use of informal speech.\n",
    " * Sentence 5: The model incorrectly predicts Italian instead of German. This could be caused by the extensive use of medical terminology that do not appear in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('da', 4.574665712489798e-30), ('de', 1.608947947062416e-33), ('it', 5.498477388514727e-39), ('es', 2.075284919539603e-40), ('en', 8.838954788761599e-37)]\n",
      "\n",
      "[('da', 1.0260356470800027e-39), ('de', 8.041484617194537e-40), ('it', 1.7188974603792168e-35), ('es', 1.947968152949224e-31), ('en', 5.688092098710772e-31)]\n",
      "\n",
      "[('da', 3.4928082009150585e-15), ('de', 1.073519587036761e-14), ('it', 2.5960233017013947e-14), ('es', 1.319101375154174e-17), ('en', 5.516637865147513e-18)]\n",
      "\n",
      "[('da', 5.548301532493822e-33), ('de', 5.08463175589846e-29), ('it', 3.4962507507199057e-25), ('es', 3.0321337800424967e-25), ('en', 3.820820840818427e-31)]\n",
      "\n",
      "[('da', 7.298071122060302e-167), ('de', 9.848431949474982e-166), ('it', 2.8836175799606796e-158), ('es', 1.857582972731872e-161), ('en', 8.190360763929044e-162)]\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"Ich habe gerade den Bug gefixt.\".lower()\n",
    "sentence_2 = \"Eduardo ha vinto 120000 Euro.\".lower()\n",
    "sentence_3 = \"Hi, what's up?\".lower()\n",
    "sentence_4 = \"Digga, nachher chillen?\".lower()\n",
    "sentence_5 = \"Diagnose: Paraoesophageale Hernie mit Upside-down-Magen, Therapie: Laparoskopische Hiatorhaphie und Gastrofundofrenicopexie\".lower()\n",
    "# https://www.juraforum.de/lexikon/fachsprache, retrieved Nov 10\n",
    "\n",
    "print(which_language(sentence_1,probability_distributions))\n",
    "print()\n",
    "print(which_language(sentence_2,probability_distributions))\n",
    "print()\n",
    "print(which_language(sentence_3,probability_distributions))\n",
    "print()\n",
    "print(which_language(sentence_4,probability_distributions))\n",
    "print()\n",
    "print(which_language(sentence_5,probability_distributions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you improve the models so that these errors do not happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is able to capture some language specific orthographical patterns through the calculation of probability distributions of bigrams of characters. However, the model is limited in several ways.\n",
    "\n",
    "First of all, it might pose a problem that the training data only consists of the bible. This makes it difficult to predict sentences from different genres (e.g. sentences with informal speech or terminology) or sentences containing loanwords. Increasing the training data and the variety of text genres might positively influence the accuracy of the model. \n",
    "\n",
    "Another shortcoming might be that the model only takes into account bigrams of characters. Looking at trigrams could be more informative as it provides more context. On the other hand, it might be too computationally expensive. Another option would be to use brigrams of tokens instead of characters. This might increase accuracy between languages with similar character patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Sentiment analysis\n",
    "\n",
    "#### Naive Bayes in movie review data\n",
    " \n",
    " * Pang, B., Lee, L., & Vaithyanathan, S. (2002, July). [Thumbs up?: sentiment classification using machine learning techniques.](http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf) In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10 (pp. 79-86). Association for Computational Linguistics.\n",
    " \n",
    " * You can download the data from [this website](http://www.cs.cornell.edu/people/pabo/movie-review-data/) (There are different versions, let's all download the 1.1 version of the polarity dataset (`polarity dataset v1.1 (2.2Mb) (includes README.1.1):...`))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filetowordlist(path, sfx):\n",
    "    words = []\n",
    "    for item in sorted(os.listdir(path)):\n",
    "        if sfx in item:\n",
    "            f=open(path + item, encoding=\"iso8859-1\")\n",
    "            lines = [line.strip() for line in f]\n",
    "            f.close()\n",
    "            wordsinfile = []\n",
    "            for l in lines:\n",
    "                sentencewords = l.split()\n",
    "                wordsinfile = wordsinfile + sentencewords\n",
    "            words.append(wordsinfile)\n",
    "    return words\n",
    "\n",
    "def log(number):\n",
    "    return np.log(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: YOU MUST ADJUST THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the directory below\n",
    "#Put the directory where you downloaded the corpus\n",
    "\n",
    "posreviews_all = filetowordlist(\"/Users/rea/Library/Mobile Documents/com~apple~CloudDocs/MSc_IT_and_Cognition/wintersemester2023/Language_Processing_I/Assignments/mix20_rand700_tokens_0211/tokens/pos/\", \".txt\")\n",
    "negreviews_all = filetowordlist(\"/Users/rea/Library/Mobile Documents/com~apple~CloudDocs/MSc_IT_and_Cognition/wintersemester2023/Language_Processing_I/Assignments/mix20_rand700_tokens_0211/tokens/neg/\", \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set 550 out of the positive reviews and 550 out of the negative reviews for creating a model\n",
    "#We will teach the model how to make predictions using that portion of the data\n",
    "posreviews_train = posreviews_all[:550]\n",
    "negreviews_train = negreviews_all[:550]\n",
    "\n",
    "#And later, we will be able to use the remaining part to see how well our model performs\n",
    "#This is called the test data\n",
    "posreviews_test  = posreviews_all[550:]\n",
    "negreviews_test  = negreviews_all[550:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 550, 144, 142)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the number of positive and negative reviews in each of the portions of the data (train and test)\n",
    "len(posreviews_train), len(negreviews_train),len(posreviews_test), len(negreviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are two lists with all words in each group of reviews\n",
    "#poswords_train contains a list of words, with concatenated positive reviews\n",
    "poswords_train=[word for sent in posreviews_train for word in sent]\n",
    "negwords_train=[word for sent in negreviews_train for word in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at a couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'novels', '\"', \"carlito's\", 'way', '\"', 'and', '\"', 'after', 'hours', '\"', 'by', 'edwin', 'torres', ')', 'starring', ':', 'al', 'pacino', ',', 'sean', 'penn', ',', 'penelope', 'ann', 'miller', ',', 'john', 'leguiziamo', ',', 'luis', 'guzman', ',', 'john', 'rebhorn', ',', 'viggio', 'mortensen', ',', 'jorge', 'porcel', \"what's\", 'shocking', 'about', '\"', \"carlito's\", 'way', '\"', 'is', 'how']\n"
     ]
    }
   ],
   "source": [
    "print (poswords_train[1000:1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'being', 'semi-consoled', 'by', 'brooke', ',', 'one', 'of', 'his', 'graduate', 'students', '.', 'he', 'lives', 'on', 'arlington', 'road', 'where', 'he', 'makes', 'friends', 'with', 'neighbors', 'oliver', '(', 'robbins', ')', 'and', 'cheryl', 'lang', '(', 'cusack', ')', '.', 'then', 'he', 'begins', 'to', 'suspect', 'that', 'oliver', 'is', 'not', 'the', 'architect', 'he', 'claims', 'to', 'be', ',']\n"
     ]
    }
   ],
   "source": [
    "print (negwords_train[1000:1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabularies for positive and negative reviews\n",
    "pos_vocab_train = set(poswords_train)\n",
    "neg_vocab_train = set(negwords_train)\n",
    "vocab_train = pos_vocab_train.union(neg_vocab_train)\n",
    "\n",
    "#Number of types (vocabulary size)\n",
    "pos_vocab_size_train = len(pos_vocab_train)\n",
    "neg_vocab_size_train = len(neg_vocab_train)\n",
    "vocab_size_train = len(vocab_train)\n",
    "\n",
    "#Number of words (tokens)\n",
    "noposwords_train=len(poswords_train)\n",
    "nonegwords_train=len(negwords_train)\n",
    "\n",
    "#Number of reviews\n",
    "noposreviews_train=len(posreviews_train)\n",
    "nonegreviews_train=len(negreviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27813, 445289, 550)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vocab_size_train,noposwords_train,noposreviews_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6094379124341003, 0.6989700043360189)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(5),np.log10(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1 (12 pts):\n",
    "Calculate prior probabilities, which are the probabilities of $P(C=positive)$ and $P(C=negative)$.\n",
    "\n",
    "Instead of saving probabilities, remember the practical issue that we mentioned at the end of the class (we save logprobs (f.ex. `np.log`) instead of regular probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the log probability of a review to be positive:\n",
      "-0.6931471805599453\n",
      "This is the log probability of a review to be negative:\n",
      "-0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "prior_probabiolity_pos_train = noposreviews_train/(noposreviews_train+nonegreviews_train)\n",
    "prior_probabiolity_neg_train = nonegreviews_train/(noposreviews_train+nonegreviews_train)\n",
    "\n",
    "print (\"This is the log probability of a review to be positive:\")\n",
    "print (log(prior_probabiolity_pos_train))\n",
    "print (\"This is the log probability of a review to be negative:\")\n",
    "print (log(prior_probabiolity_neg_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tricky part:\n",
    "\n",
    "Now we need to estimate the probability of a given word, given a specific class.\n",
    "\n",
    "$$P\\left(w_{k} | c_{i}\\right)=\\frac{\\operatorname{count}\\left(w_{k}, c_{i}\\right)+1}{\\sum_{w \\in V} \\operatorname{count}\\left(w, c_{i}\\right) + |V|}$$\n",
    "\n",
    "$$ logP\\left(w_{k} | c_{i}\\right) = log (P\\left(w_{k} | c_{i}\\right))$$\n",
    "\n",
    "We need some word counts, then.\n",
    "\n",
    "#### Exercise 3.2 (12 pts):\n",
    "Calculate word counts for the positive reviews and save them in a variable.\n",
    "\n",
    "Calculate also the word counts for the negative reviews and save them in another variable.\n",
    "\n",
    "##### Hint: You can use a dictionary (for each document class or polarity) to save the word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word nice appears 118 times in the positive reviews\n",
      "The word nice appears 92 times in the negative reviews\n",
      "\n",
      "The word bad appears 223 times in the positive reviews\n",
      "The word bad appears 509 times in the negative reviews\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "from collections import Counter\n",
    "pos_frequencies = Counter(poswords_train)\n",
    "neg_frequencies = Counter(negwords_train)\n",
    "\n",
    "print (\"The word nice appears \"+str(pos_frequencies[\"nice\"])+\" times in the positive reviews\")\n",
    "print (\"The word nice appears \"+str(neg_frequencies[\"nice\"])+\" times in the negative reviews\")\n",
    "\n",
    "print ()\n",
    "\n",
    "print (\"The word bad appears \"+str(pos_frequencies[\"bad\"])+\" times in the positive reviews\")\n",
    "print (\"The word bad appears \"+str(neg_frequencies[\"bad\"])+\" times in the negative reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3 (12 pts):\n",
    "Now estimate the probability of each word to appear in a positive review. Do the same with negative reviews. When you save them, do as you did in the first exercise, and save log probabilities.\n",
    "\n",
    "##### Hint: You can use a dictionary (for each document class or polarity) to save the word logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log probability of the word nice to appear in the positive reviews is: -8.310737353530826\n",
      "The log probability of the word nice to appear in the negative reviews is: -8.438475770037291\n",
      "\n",
      "The log probability of the word bad to appear in the positive reviews is: -7.678214794787315\n",
      "The log probability of the word bad to appear in the negative reviews is: -6.736664537472175\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "pos_logprobs = {key:np.log((value+1)/(vocab_size_train + noposwords_train)) for key,value in pos_frequencies.items()}\n",
    "neg_logprobs = {key:np.log((value+1)/(vocab_size_train + nonegwords_train)) for key,value in neg_frequencies.items()}\n",
    "\n",
    "\n",
    "print (\"The log probability of the word nice to appear in the positive reviews is: \"+str(pos_logprobs[\"nice\"]))\n",
    "print (\"The log probability of the word nice to appear in the negative reviews is: \"+str(neg_logprobs[\"nice\"]))\n",
    "\n",
    "print ()\n",
    "\n",
    "print (\"The log probability of the word bad to appear in the positive reviews is: \"+str(pos_logprobs[\"bad\"]))\n",
    "print (\"The log probability of the word bad to appear in the negative reviews is: \"+str(neg_logprobs[\"bad\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.4 (12 pts):\n",
    "In order to have a clean implementation, estimate the log probability of a word that doesn't appear in the training set.\n",
    "\n",
    "This will make the code of the next exercise cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log probability of an out of vocabulary word in the positive reviews is:\n",
      "-13.089860846642354\n",
      "The log probability of an out of vocabulary word in the negative reviews is:\n",
      "-12.971075263190547\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "pos_oov_word_logprob = log(1/(vocab_size_train+noposwords_train))\n",
    "neg_oov_word_logprob = log(1/(vocab_size_train+nonegwords_train)) \n",
    "\n",
    "print (\"The log probability of an out of vocabulary word in the positive reviews is:\")\n",
    "print (pos_oov_word_logprob)\n",
    "print (\"The log probability of an out of vocabulary word in the negative reviews is:\")\n",
    "print (neg_oov_word_logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.5 (12 pts):\n",
    "\n",
    "Now we just need to write a function that will return True if an input review is positive, and False if the input review is negative.\n",
    "\n",
    "$$\\operatorname{argmax}\\left(\\log P\\left(c_{i}\\right)+\\sum_{k=1}^{N} \\log P\\left(w_{k} | c_{i}\\right)\\right)$$\n",
    "\n",
    "How would you do that?\n",
    "\n",
    "##### Hint: Keep it simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_or_not(s):\n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    prob_pos = log(prior_probabiolity_pos_train) # start with class probability for positive class\n",
    "    prob_neg = log(prior_probabiolity_neg_train) # start with class probability for negative class\n",
    "    \n",
    "    for word in s: \n",
    "        if word in pos_logprobs: \n",
    "            prob_pos += pos_logprobs[word] # add conditional word probability to sum\n",
    "        else:\n",
    "            pos_oov_word_logprob \n",
    "            '''if word not in training data add log probability of a word \n",
    "            in the positive reviews that doesn't appear in the training set'''\n",
    "    \n",
    "    for word in s:\n",
    "        if word in neg_logprobs: \n",
    "            prob_neg += neg_logprobs[word] # add conditional word probability to sum\n",
    "        else:\n",
    "            neg_oov_word_logprob \n",
    "            '''if word not in training data add log probability of a word in the negative \n",
    "            reviews that doesn't appear in the training set'''\n",
    "\n",
    "    if prob_pos > prob_neg: \n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"the movie is nice\".split()\n",
    "positive_or_not(review_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"the movie is bad\".split()\n",
    "positive_or_not(review_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"such an awful movie\".split()\n",
    "positive_or_not(review_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"lovely experience\".split()\n",
    "positive_or_not(review_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
