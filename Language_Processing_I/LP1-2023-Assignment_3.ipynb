{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Processing assignment 3: Word embeddings and society\n",
    "\n",
    "In this assignment you will have to load vectorial representations of words and calculate their cosine similarity, a common distance metric to evaluate semantic similarity.\n",
    "\n",
    "On grading: There are five exercises in this assignment. You must have at least three correct exercises (and among the incorrect ones, there should be some proper attempt to solve the missing exercises). What we mean is that if you do three perfect exercises but the remaining two exercises are blank, the assignment will not be considered passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Exercise 1:\n",
    " \n",
    "In order to play with word embeddings, we need a way of storing them in our program. We need a data structure to represent all the word embeddings.\n",
    "\n",
    "The goal of this exercise is to open the file where the embeddings are saved and to put them in a variable that you can use afterwards.\n",
    "\n",
    "You can represent the data in the way that you think it fits best. The result can go from a really simple approach until a complex but useful class.\n",
    "\n",
    "Given a word, such as `\"house\"`, this data structure should return the embeddings related to that word.\n",
    "\n",
    "In the saved file, we will have a set of words, and each word will be represented as a sequence of floating point numbers, such as:\n",
    "\n",
    "`house --> 0.001 0.002 0.005 0.001 0.0012312 0.004 ...`\n",
    "\n",
    "`cow --> 0.2 0.01 0.00031 0.01 0.9 0.00031 0.0015 0.002 ...`\n",
    "\n",
    "The number of floating point numbers will always be the same.\n",
    "\n",
    "---\n",
    " \n",
    "If we want to play with word embeddings, we have to get them from somewhere. Pick English embeddings from [Absalon](https://absalon.ku.dk/files/7371057/download?download_frd=1) or the embeddings that you want from this [website](https://fasttext.cc/docs/en/crawl-vectors.html). I recommend you downloading the file from Absalon, as it contains only 50,000 words and it is easier to load (well, faster).\n",
    "\n",
    "If you get the embeddings from the Github page, you should download the embeddings in **text** format. These embeddings have been trained with raw text from Wikipedia. This may take some space in your computer, depending on the language you choose.\n",
    "\n",
    "Once you downloaded the embeddings, it's time to start programming! The files follow a specific format.\n",
    "\n",
    "##### FILE FORMAT:\n",
    "\n",
    "The first line in the file contains two numbers separated by a single space. The first number indicates the number of words in the file (`N_WORDS`) and the second number specifies the number of dimensions (`N_DIMENSIONS`) that are used to represent each of those words.\n",
    "\n",
    "After the first line, each line will contain one word at the beginning. Following the word, and separated by spaces, there will be `N_DIMENSIONS` numbers, which represent each word in the space.\n",
    "\n",
    "The words are sorted by their frequency in the wikipedia corpus, then the first words in the file will be the most frequent ones. Here you can see how the English embeddings file starts:\n",
    "\n",
    "`9999 300`\n",
    "\n",
    "`, 0.1250 -0.1079 0.0245 -0.2529 0.1057 -0.0184 0.1177 ...`\n",
    "\n",
    "`the -0.0517 0.0740 -0.0131 0.0447 -0.0343 0.0212 0.0069 ...`\n",
    "\n",
    "`. 0.0342 -0.0801 0.1162 -0.3968 -0.0147 -0.0533 0.0606 ...`\n",
    "\n",
    "`and 0.0082 -0.0899 0.0265 -0.0086 -0.0609 0.0068 0.0652 ...`\n",
    "\n",
    "`...`\n",
    "\n",
    "##### What you have to do:\n",
    "\n",
    "Write a program to read the file and store the words and their embeddings in the data structure that you think it is the best. It might be very simple, or it might be a more complex one.\n",
    "\n",
    "##### Important note: You are not allowed to use a package like gensim to open the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49997\n"
     ]
    }
   ],
   "source": [
    "#1.- Define object to save words and their embeddings\n",
    "#2.- Write code for reading the file and save it in the defined object\n",
    "#YOUR CODE HERE\n",
    "words = dict()\n",
    "with open(\"wiki.en.vec.short50K\") as f:\n",
    "    f.readline() # skip first line\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        vector = np.array(line[1:], dtype=float) \n",
    "        if len(vector) == 300: # only keep vectors of size 300\n",
    "            word = line[0] \n",
    "            words[word] = vector # add word and vector to words dictionary\n",
    "print(len(words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:\n",
    "\n",
    "A common distance metric used to measure the similarity between two words is the cosine similarity, which measures the cosine of the angle between the two vectors that represent each of the words.\n",
    "\n",
    "This similarity value is calculated by using this formula:\n",
    "\n",
    "$$\\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\|_2 \\|\\mathbf{B}\\|_2} $$\n",
    "\n",
    "<!--= \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }-->\n",
    "\n",
    "Don't be scared. The first part of the formula, $\\mathbf{A} \\cdot \\mathbf{B}$ is the dot product between vectors $\\mathbf{A}$ and $\\mathbf{B}$. And you know how to do that in Python.\n",
    "\n",
    "$\\mathbf{A} \\cdot \\mathbf{B} = \\sum\\limits_{i=1}^{n}{A_i  B_i}$\n",
    "\n",
    "In the lower part, $\\|\\mathbf{A}\\|_2 \\|\\mathbf{B}\\|_2$, you have to calculate the Euclidean norm of each vector ($\\mathbf{A}$ and $\\mathbf{B}$) and multiply their results. The Euclidean norm is calculated using this formula:\n",
    "\n",
    "$\\|\\mathbf{A}\\|_2 = \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}$\n",
    "\n",
    "The formula inside the square root is the same as the one from the dot product. Then it can be rewritten like this:\n",
    "\n",
    "$\\|\\mathbf{A}\\|_2 = \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}} = \\sqrt{\\mathbf{A} \\cdot \\mathbf{A}} $\n",
    "\n",
    "You should program the cosine similarity function by using numpy. You cannot use previously programmed cosine similarity functions, you must write your own function. This program must get two numpy arrays and it should return a number.\n",
    "\n",
    "The resulting number of this formula should be interpreted as a number that specifies the similarity between two words. The higher the number, the similarity between those two words will be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5719145601185748"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similarity(A, B):\n",
    "    #YOUR CODE HERE\n",
    "    return np.dot(A,B)/(np.sqrt(np.dot(A,A))*np.sqrt(np.dot(B,B)))\n",
    "\n",
    "similarity(words[\"and\"],words[\"but\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3:\n",
    "\n",
    "In the third exercise you have to squeeze your brain a bit more. Now, you have loaded the whole embedding file, and you also have a distance metric to measure the similarity between words. Let's do more complex things, then.\n",
    "\n",
    "Given a word, you have to find the 30 most similar words. Then, given one word you should get the distance to all the words in the embeddings file, and pick the nearest ones.\n",
    "\n",
    "In order to make this task easier, I attach a simple implementation of an ordered list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function should return the embeddings of a word according to your class\n",
    "def get (LIST, index):\n",
    "    return LIST[index]\n",
    "\n",
    "def get_value(el):\n",
    "    return el[1]\n",
    "\n",
    "\n",
    "\n",
    "class OrderedListTuple:\n",
    "    \n",
    "    def __init__(self, max_size):\n",
    "        self.content = []\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def find_pos (self, element):\n",
    "        index = 0\n",
    "        while (index <= len(self.content)-1) and get_value(get(self.content, index)) > get_value(element):\n",
    "            index += 1\n",
    "        return index\n",
    "\n",
    "    def insert_element (self, element):\n",
    "        pos = self.find_pos (element)\n",
    "        self.content.insert (pos, element)\n",
    "        if len(self.content) > self.max_size:\n",
    "            self.content.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is very simple. When we initialize the list, we set the number of elements that it will have at most. Then, when we add elements to the list, it will add the element in the correct position. But, if the number of elements is higher than the ones that we can keep, the object will remove the last element. Let's see how it works with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[('house', 14)]\n",
      "[('house', 14), ('home', 6)]\n",
      "[('house', 14), ('home', 6), ('brown', 3)]\n",
      "[('house', 14), ('home', 6), ('elbow', 4), ('brown', 3)]\n",
      "[('house', 14), ('home', 6), ('elbow', 4), ('brown', 3)]\n",
      "[('house', 14), ('the', 9), ('home', 6), ('elbow', 4)]\n",
      "[('and', 43), ('house', 14), ('the', 9), ('home', 6)]\n",
      "[('kitty', 44), ('and', 43), ('house', 14), ('the', 9)]\n"
     ]
    }
   ],
   "source": [
    "L = OrderedListTuple(4)\n",
    "print (L.content)\n",
    "\n",
    "L.insert_element((\"house\", 14))\n",
    "print (L.content)\n",
    "L.insert_element((\"home\", 6))\n",
    "print (L.content)\n",
    "L.insert_element((\"brown\", 3))\n",
    "print (L.content)\n",
    "L.insert_element((\"elbow\", 4))\n",
    "print (L.content)\n",
    "L.insert_element((\"high\", 1))\n",
    "print (L.content)\n",
    "L.insert_element((\"the\", 9))\n",
    "print (L.content)\n",
    "L.insert_element((\"and\", 43))\n",
    "print (L.content)\n",
    "L.insert_element((\"kitty\", 44))\n",
    "print (L.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: Why don't you create a similarity function that gets two words, and it returns a tuple? For each word in the dictionary, you can calculate the similarity to an input word, and save this in a tuple. Then, using the previous data structure, you can save only the N-best words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data structure, you should be able to get the most similar words to one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 1.0),\n",
       " ('both', 0.7088399055288035),\n",
       " ('well', 0.6507605201793852),\n",
       " ('including', 0.6455951558139696),\n",
       " ('while', 0.6344329391819657),\n",
       " ('lastly', 0.6124539704969483),\n",
       " ('additionally', 0.6103882770258247),\n",
       " ('the', 0.60341005556975),\n",
       " ('of', 0.600915614446954),\n",
       " ('which', 0.599827363344629),\n",
       " (',', 0.5975515358044979),\n",
       " ('along', 0.5920798889560756),\n",
       " ('notably', 0.5828809034774284),\n",
       " ('also', 0.5823754688131197),\n",
       " ('addition', 0.5804041956874957),\n",
       " ('other', 0.5773918231698155),\n",
       " ('but', 0.5719145601185748),\n",
       " ('likewise', 0.5702313752171171),\n",
       " ('however', 0.5635267429063764),\n",
       " ('with', 0.5604893133079214),\n",
       " ('especially', 0.5600702339008268),\n",
       " ('respectively', 0.5507238976794473),\n",
       " ('these', 0.5502342758557172),\n",
       " ('consequently', 0.546646170769302),\n",
       " ('besides', 0.545569494198823),\n",
       " ('many', 0.5448865850382615),\n",
       " ('similarly', 0.543413204922898),\n",
       " ('several', 0.5432807476331165),\n",
       " ('although', 0.5431728976115298),\n",
       " ('furthermore', 0.5383040086371182)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "def similarity_tuple(word1,word2,dictionary):\n",
    "    '''\n",
    "    Calculate similarity between two words based on their vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - word1 (str): First word.\n",
    "    - word2 (str): Second word.\n",
    "    - dictionary (dict): A dictionary that maps words to vectors.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the second word and the similarity score to the first word.\n",
    "    '''\n",
    "    vector1 = dictionary[word1]\n",
    "    vector2 = dictionary[word2]\n",
    "    return (word2,similarity(vector1,vector2))\n",
    "\n",
    "def most_similar_words(input_word,dictionary,max_size):\n",
    "    '''\n",
    "    Find the most similar words to a given input word based on similarity between vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_word (str): The input word.\n",
    "    - dictionary (dict): A dictionary that maps words to vectors.\n",
    "    - max_size (int): The number of similar words to retrieve.\n",
    "    '''\n",
    "    L = OrderedListTuple(max_size)\n",
    "    for word,vector in dictionary.items():\n",
    "        similarity = similarity_tuple(input_word,word,dictionary) # get similarity tuple\n",
    "        L.insert_element(similarity) # insert similarity tuple to L\n",
    "    return L.content\n",
    "\n",
    "most_similar_words(\"and\",words,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4:\n",
    "\n",
    "The last exercise is really cool. One of the properties that researchers found in word embeddings was that we could perform algebraic operations over the vectors in order to get specific words.\n",
    "\n",
    "For example, if we perform an operation like this one:\n",
    "\n",
    "$$DICTIONARY['berlin'] - DICTIONARY['germany'] + DICTIONARY['france']$$\n",
    "\n",
    "This results in a vector. If we find the 20 closest words to that vector, we should be able to see that the word `Germany` will be near. Another nice operation was:\n",
    "\n",
    "$$DICTIONARY['queen'] - DICTIONARY['woman'] + DICTIONARY['man']$$\n",
    "\n",
    "Perform this operations with the words you want, and check if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paris', 0.7745821281649259),\n",
       " ('berlin', 0.6806360501069306),\n",
       " ('france', 0.6234364339620112),\n",
       " ('marseille', 0.6068132959916012),\n",
       " ('toulouse', 0.6053895238433366),\n",
       " ('montpellier', 0.5917006165578014),\n",
       " ('rouen', 0.5914574790560799),\n",
       " ('avignon', 0.5890789796674709),\n",
       " ('rennes', 0.5850185342314214),\n",
       " ('ferrand', 0.5844373101108589),\n",
       " ('brussels', 0.5783981772734191),\n",
       " ('nantes', 0.5750482980258821),\n",
       " ('bordeaux', 0.5732881366656535),\n",
       " ('marseilles', 0.5714683689465664),\n",
       " ('boulogne', 0.5677972274128383),\n",
       " ('neuilly', 0.5665287787178916),\n",
       " ('grenoble', 0.5660398964645867),\n",
       " ('lille', 0.5642594745563221),\n",
       " ('provence', 0.5618433958267668),\n",
       " ('poitiers', 0.558248266369883)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "# modify functions from above so they take vectors as input and not words\n",
    "def similarity_tuple_vec(vec1,word2,dictionary):\n",
    "    vec2 = dictionary[word2]\n",
    "    return (word2,similarity(vec1,vec2))\n",
    "\n",
    "def most_similar_words_vec(input_vector,dictionary,max_size):\n",
    "    L = OrderedListTuple(max_size)\n",
    "    for word,vector in dictionary.items():\n",
    "        similarity = similarity_tuple_vec(input_vector,word,dictionary)\n",
    "        L.insert_element(similarity)\n",
    "    return L.content\n",
    "\n",
    "\n",
    "result = words['berlin'] - words['germany'] + words['france'] #berlin - germany + france\n",
    "most_similar_words_vec(result,words,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7898672082967786),\n",
       " ('king', 0.6429407513059967),\n",
       " ('majesty', 0.5372264198483585),\n",
       " ('monarch', 0.5126070187949001),\n",
       " ('crown', 0.469760687497465),\n",
       " ('queens', 0.4569217657524951),\n",
       " ('whitehall', 0.4508493631090601),\n",
       " ('reign', 0.4498360282763127),\n",
       " ('coronation', 0.44729696911956957),\n",
       " ('royal', 0.43049023386023016),\n",
       " ('regent', 0.4278365170631544),\n",
       " ('jubilee', 0.42361938568326163),\n",
       " ('kings', 0.42048129263683504),\n",
       " ('prince', 0.4184276772487855),\n",
       " ('connaught', 0.4147187331471937),\n",
       " ('consort', 0.4101795096405898),\n",
       " ('princess', 0.4090483186856231),\n",
       " ('throne', 0.40795276082669685),\n",
       " ('pretender', 0.40745142030398634),\n",
       " ('elizabeth', 0.4070293809206282)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "#queen - woman + man\n",
    "result = words['queen'] - words['woman'] + words['man']\n",
    "most_similar_words_vec(result,words,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5:\n",
    "\n",
    "In recent years, many researchers have shown that word embeddings obtained from large corpora reproduce biases that happen in society.\n",
    "\n",
    "In this exercise, we would like to ask you to try to show some examples in the loaded word embedding file that show some sort of bias. This bias can be either of these, or any other bias you are interested:\n",
    "\n",
    " * Gender\n",
    " * Origin\n",
    " * Sexual preference\n",
    " * Socioeconomic class\n",
    " * Academic background\n",
    " \n",
    "These examples could be based on distances between words, but any other creative methodology that you could think of will be well considered as well.\n",
    "\n",
    "For instance, what is the distance between \"maid\" and \"man\", and \"maid\" and \"woman\"?\n",
    "\n",
    "You should provide examples but also your interpretation of these results.\n",
    "\n",
    "If you want to get some inspiration, you may want to check some recent articles about the topic:\n",
    "\n",
    "  * Bender, Emily M., and Batya Friedman. \"Data statements for natural language processing: Toward mitigating system bias and enabling better science.\" Transactions of the Association for Computational Linguistics 6 (2018): 587-604. https://aclanthology.org/Q18-1041/\n",
    "  * Hovy, Dirk, and Shrimai Prabhumoye. \"Five sources of bias in natural language processing.\" Language and Linguistics Compass 15.8 (2021): e12432. https://compass.onlinelibrary.wiley.com/doi/10.1111/lnc3.12432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nurse, man:  0.21655707042157213\n",
      "nurse, woman:  0.48998861722812437\n",
      "\n",
      "boss, man:  0.41120083668655244\n",
      "boss, woman:  0.2984269239418242\n",
      "\n",
      "scientist, American:  0.2633526803036764\n",
      "scientist, African:  0.1314031374973034\n",
      "\n",
      "horrible, Germany:  0.15864924391812563\n",
      "horrible, Netherlands:  0.1064987566348144\n",
      "\n",
      "scientist, American:  0.2633526803036764\n",
      "scientist, African:  0.1314031374973034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "# gender bias (based on distance between words)\n",
    "\n",
    "print(\"nurse, man: \", similarity(words[\"nurse\"],words[\"man\"]))\n",
    "print(\"nurse, woman: \", similarity(words[\"nurse\"],words[\"woman\"])) \n",
    "print()\n",
    "print(\"boss, man: \", similarity(words[\"boss\"],words[\"man\"]))\n",
    "print(\"boss, woman: \", similarity(words[\"boss\"],words[\"woman\"]))\n",
    "print()\n",
    "\n",
    "# ethnicity/origin bias (based on distance between words)\n",
    "\n",
    "print(\"scientist, American: \",similarity(words[\"scientist\"],words[\"american\"]))\n",
    "print(\"scientist, African: \", similarity(words[\"scientist\"],words[\"african\"]))\n",
    "print()\n",
    "print(\"horrible, Germany: \", similarity(words[\"horrible\"],words[\"germany\"])) # due to historical events, one country has higher sim. with negative words\n",
    "print(\"horrible, Netherlands: \", similarity(words[\"horrible\"],words[\"netherlands\"]))\n",
    "print()\n",
    "\n",
    "print(\"scientist, American: \",similarity(words[\"scientist\"],words[\"american\"]))\n",
    "print(\"scientist, African: \", similarity(words[\"scientist\"],words[\"african\"]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR INTERPRETATION HERE (press ENTER to write)\n",
    "\n",
    "As indicated by differences in word similarities, the loaded word embedding file shows some forms of gender and ethnicity/origin bias.\n",
    "\n",
    "Regarding gender bias, the embedding of \"nurse\" is more similar to the embedding of \"woman\" than to the embedding of \"man\". Another example concerns the higher similarity of \"boss\" and \"man\" than the similarity of \"boss\" and \"woman\". This emphasizes that the dataset reflects stereotypical and historical associations of professions and gender.\n",
    "\n",
    "Moreover, instances of origin or ethnicity bias could be detected: the embedding for \"scientist\" is more similar to the embedding of \"American\" than to the embedding of \"African\". This could be explained by an overrepresentation of certain cultures and regions in the training data, such as a higher amount of entries describing the discoveries of American scientists. Another example might be the higher similarity of the word pair \"horrible\"-\"Germany\" compared to the similarity of \"horrible\"-\"Netherlands\". This could be due to negative historical events described in several wikipedia entries. Even though the data was trained on a Wikipedia Corpus and thus consists of texts that aim to describe topics in a more neutral way, biases can still emerge due to, among other factors, the biases of authors of the articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "In this exercise we are going to see how to calculate word counts normalized by document frequency (TF-IDF).\n",
    "\n",
    "To this end, we will calculate the word frequencies and the IDF normalized counts using a Python package called TFIDFvectorizer from Scikit-Learn.\n",
    "\n",
    "We will work with the Gutenberg corpus from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileids = gutenberg.fileids()\n",
    "fileids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for fileid in fileids:\n",
    "    corpus.append(nltk.corpus.gutenberg.raw(fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he 0.08135534295322783\n",
      "his 0.08280811693453548\n",
      "thee 0.08905556490592986\n",
      "with 0.09588308276630424\n",
      "my 0.1205802404485341\n",
      "to 0.16125791192514802\n",
      "in 0.20484113136437723\n",
      "of 0.21210500127091542\n",
      "and 0.5055653454950587\n",
      "the 0.6377677777940539\n"
     ]
    }
   ],
   "source": [
    "#Step 1:\n",
    "vec_tfidf = TfidfVectorizer()\n",
    "result_tfidf = vec_tfidf.fit_transform(corpus).toarray()\n",
    "\n",
    "#Step 2:\n",
    "id2word = {vec_tfidf.vocabulary_[key]:key for key in vec_tfidf.vocabulary_.keys()}\n",
    "\n",
    "#Step 3:\n",
    "sorted_ids_blake = np.argsort(result_tfidf[4]).reshape(-1)\n",
    "\n",
    "#Step 4:\n",
    "for id in sorted_ids_blake[-10:]:\n",
    "    print (id2word[id],result_tfidf[4][id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6.1:\n",
    "Explain using your own words and in one single sentence (per step) what happens in each step.\n",
    "\n",
    "###### Step 1: A TfidfVectorizer is applied to the corpus (list of raw texts from the Gutenberg corpus), thereby creating a TF-IDF matrix (rows=documents, columns=words).\n",
    "\n",
    "###### Step 2: A dictionary mapping feature indices to terms is created. \n",
    "\n",
    "###### Step 3: The indices of the words of  the 5th document ('blake-poems.txt') are sorted in ascending order by TF-IDF score and stored in a 1D array.\n",
    "\n",
    "###### Step 4: The 10 words with the highest TF-IDF score in the 5th document are printed along with their TF-IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.2:\n",
    "Can you check what are the top-10 most relevant words based on their inverse document frequency? I am asking for the 10 words with the highest inverse document frequency.\n",
    "\n",
    "If you do not know how to get the IDFs of the words, you may want to take a look at the documentation of the TFIDF vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hummingly 3.2512917986064953\n",
      "hummer 3.2512917986064953\n",
      "hummed 3.2512917986064953\n",
      "humiliations 3.2512917986064953\n",
      "humiliates 3.2512917986064953\n",
      "humh 3.2512917986064953\n",
      "humboldt 3.2512917986064953\n",
      "humbling 3.2512917986064953\n",
      "humblest 3.2512917986064953\n",
      "zuzims 3.2512917986064953\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "idfs = vec_tfidf.idf_ # get IDFs of words\n",
    "\n",
    "sorted_ids_idfs = np.argsort(idfs) # sort ids of words by IDF score\n",
    "\n",
    "for i in sorted_ids_idfs[-10:]:\n",
    "    print (id2word[i],idfs[i]) # print the 10 words with highest IDF scores along with their IDF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2512917986064953\n"
     ]
    }
   ],
   "source": [
    "idf_score = math.log((1 + 18) / (1 + 1)) + 1 # check value for hapax legomena\n",
    "# formula from: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html, retrieved 9th of Dec 2023\n",
    "print(idf_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.3:\n",
    "\n",
    "How do the inverse document frequencies look like in this corpus? Do they seem relevant? Please state that in 1-2 sentences.\n",
    "\n",
    "#### YOUR RESPONSE HERE\n",
    "The 10 words with highest inverse document frequency all have a value of 3.25, suggesting that they are hapax legomena, so they only appear in one document. If our search query would be \"Humboldt\" then we would like to retrieve the only document mentioning \"Humboldt\", so we want to assign a higher weight to that term since it is highly informative. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
